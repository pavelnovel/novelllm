{"timestamp": "2024-09-07 00:47:52", "text": "My approach to AI prompting has changed a lot over the last year. I used to try to optimize my prompts to get the output I needed in the first try. I had a collection of massive starting prompts that had all these parameters and context. When I talk to an LLM now, I write like I'm DRUNK TEXTING. Misspellings, incomplete thoughts, and fragmented sentences. I'll say things like \"ga4 - reporting on page visits how to help me\" to get the conversation going. (This is actually how I learned to use GA4). The reason is that I know I will need to give the LLM more feedback and guidance beyond the first try, no matter how much I try to optimize for zero-shot prompts. I'd rather get the conversation going and see an output to give feedback to rather than overload it with information and overload myself with a barrier. I tell people this, too. They will get so much more value out of it with a 50% effective than a 100% effective prompt that is too unwieldy to use in the middle of work. Overthinking the first prompt will stop people from actually using the tools. The value of AI really emerges through the conversation and the iterations of feedback within the chat. My goal now is to get the right output on the 20th response of a chat, not the first."}